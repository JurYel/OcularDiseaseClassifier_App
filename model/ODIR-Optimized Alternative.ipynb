{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dietary-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "satellite-accent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../assets/datasets/odir5k\\ODIR-5K\n",
      "../../assets/datasets/odir5k\\preprocessed_images\n",
      "../../assets/datasets/odir5k\\ODIR-5K\\ODIR-5K\n",
      "../../assets/datasets/odir5k\\ODIR-5K\\ODIR-5K\\Testing Images\n",
      "../../assets/datasets/odir5k\\ODIR-5K\\ODIR-5K\\Training Images\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../../assets/datasets/odir5k\"\n",
    "\n",
    "for folder, subfolders, files in os.walk(base_dir):\n",
    "    for subf in subfolders:\n",
    "        print(os.path.join(folder, subf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "several-invitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  7000\n",
      "test: 1000\n"
     ]
    }
   ],
   "source": [
    "train_path = \"../../assets/datasets/odir5k/ODIR-5K/ODIR-5K/Training Images\"\n",
    "test_path = \"../../assets/datasets/odir5k/ODIR-5K/ODIR-5K/Testing Images\"\n",
    "disc_path = \"../../assets/datasets/odir5k/DiscardedImages.csv\"\n",
    "annot_path = \"../../assets/datasets/odir5k/full_df.csv\"\n",
    "train_files = sorted(os.listdir(train_path))\n",
    "test_files = sorted(os.listdir(test_path))\n",
    "\n",
    "print('train: ', len(train_files))\n",
    "print('test:', len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "golden-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "\n",
    "import shutil\n",
    "\n",
    "def clear_content(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as exc:\n",
    "            raise(\"Failed to remove: %s %s\" % (file_path, exc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minute-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper classes\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from skimage import exposure\n",
    "\n",
    "class ImageCrop:\n",
    "    '''Cropping Images'''\n",
    "    def __init__(self, source_folder, destination_folder, file_name):\n",
    "        self.source_folder = source_folder\n",
    "        self.destination_folder = destination_folder\n",
    "        self.file_name = file_name\n",
    "        \n",
    "    def remove_black_pixels(self):\n",
    "        file = os.path.join(self.source_folder, self.file_name)\n",
    "        image = cv2.imread(file)\n",
    "        \n",
    "        # mask of colored pixels\n",
    "        mask = image > 0\n",
    "        \n",
    "        # coorindates of colored pixels\n",
    "        coordinates = np.argwhere(mask)\n",
    "        \n",
    "        # binding box of non-black pixels\n",
    "        x0, y0, s0 = coordinates.min(axis=0)\n",
    "        x1, y1, s1 = coordinates.max(axis=0) + 1\n",
    "        \n",
    "        # crop contents of bounding box\n",
    "        cropped = image[x0:x1, y0:y1]\n",
    "        \n",
    "        file_cropped = os.path.join(self.destination_folder, self.file_name)\n",
    "        cv2.imwrite(file_cropped, cropped)\n",
    "    \n",
    "    \n",
    "class ImageResizer:\n",
    "    '''Resize Image'''\n",
    "    def __init__(self, image_width, quality, source_folder, \n",
    "                 destination_folder, file_name, keep_aspect_ratio):\n",
    "        self.image_width = image_width\n",
    "        self.quality = quality\n",
    "        self.source_folder = source_folder\n",
    "        self.destination_folder = destination_folder\n",
    "        self.file_name = file_name\n",
    "        self.keep_aspect_ratio = keep_aspect_ratio\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Runst the image library using the constructor arguments.\n",
    "        Args:\n",
    "            No arguments required.\n",
    "        Returns:\n",
    "            Saves the treated image into a separate folder.\n",
    "        \"\"\"\n",
    "\n",
    "        # We load the original file, we resize it to a smaller width and corresponding height and\n",
    "        # also mirror the image when we find a right eye image so they are all left eyes.\n",
    "        \n",
    "        file = os.path.join(self.source_folder, self.file_name)\n",
    "        img = Image.open(file)\n",
    "        if self.keep_aspect_ratio:\n",
    "            # it will have the exact same width-to-height ratio as the original\n",
    "            width_perc = (self.image_width / float(img.size[0]))\n",
    "            height_size = int((float(img.size[1] * float(width_perc))))\n",
    "            img = img.resize((self.image_width, height_size), PIL.Image.ANTIALIAS)\n",
    "        else:\n",
    "            # This will force the image to be square\n",
    "            img = img.resize((self.image_width, self.image_width), PIL.Image.ANTIALIAS)\n",
    "        if \"right\" in self.file_name:\n",
    "            img.transpose(Image.FLIP_LEFT_RIGHT).save(os.path.join(self.destination_folder, \n",
    "                                                                   self.file_name), \n",
    "                                                      optimize=True, quality=self.quality)\n",
    "        else:\n",
    "            img.save(os.path.join(self.destination_folder, self.file_name))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-netscape",
   "metadata": {},
   "source": [
    "**Image Cropping - Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "higher-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image croppping job - training\n",
    "def crop_all_images(source_folder,destination_folder):\n",
    "    files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
    "    for file in files:\n",
    "        if file not in os.listdir(destination_folder):\n",
    "            ImageCrop(source_folder, destination_folder, file).remove_black_pixels()\n",
    "\n",
    "source_folder = train_path\n",
    "destination_folder = \"ODIR-5K_Training_Dataset_cropped\"\n",
    "\n",
    "if os.path.exists(destination_folder):\n",
    "    if len(os.listdir(destination_folder)) < 500:\n",
    "        clear_content(destination_folder)\n",
    "else:\n",
    "    os.mkdir(destination_folder)\n",
    "\n",
    "crop_all_images(source_folder, destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-participant",
   "metadata": {},
   "source": [
    "**Image Cropping - Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "every-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image croppping job - testing\n",
    "def crop_all_images(source_folder,destination_folder):\n",
    "    files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
    "    for file in files:\n",
    "        if file not in os.listdir(destination_folder):\n",
    "            ImageCrop(source_folder, destination_folder, file).remove_black_pixels()\n",
    "\n",
    "source_folder = test_path\n",
    "destination_folder = \"ODIR-5K_Testing_Dataset_cropped\"\n",
    "\n",
    "if os.path.exists(destination_folder):\n",
    "    if len(os.listdir(destination_folder)) < 500:\n",
    "        clear_content(destination_folder)\n",
    "else:\n",
    "    os.mkdir(destination_folder)\n",
    "\n",
    "crop_all_images(source_folder, destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-excuse",
   "metadata": {},
   "source": [
    "**Image Treatment Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "included-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image treatment training\n",
    "def resize_all_images(source_folder, destination_folder, image_width, quality, keep_aspect_ratio):\n",
    "    files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
    "    for file in files:\n",
    "        if file not in os.listdir(destination_folder):\n",
    "            ImageResizer(image_width, quality, source_folder, destination_folder, file, keep_aspect_ratio).run()\n",
    "    \n",
    "image_width = 224\n",
    "keep_aspect_ratio = False\n",
    "quality = 100\n",
    "source_folder = \"ODIR-5K_Training_Dataset_cropped\"\n",
    "destination_folder = \"ODIR-5K_Training_Dataset_treated\" + '_' + str(image_width)\n",
    "\n",
    "if os.path.exists(destination_folder):\n",
    "    if len(os.listdir(destination_folder)) < 500:\n",
    "        clear_content(destination_folder)\n",
    "else:\n",
    "    os.mkdir(destination_folder)\n",
    "    \n",
    "resize_all_images(source_folder, destination_folder, image_width, quality, keep_aspect_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-grenada",
   "metadata": {},
   "source": [
    "**Image Treatment Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "processed-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image treatment testing\n",
    "def resize_all_images(source_folder, destination_folder, image_width, quality, keep_aspect_ratio):\n",
    "    files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
    "    for file in files:\n",
    "        if file not in os.listdir(destination_folder):\n",
    "            ImageResizer(image_width, quality, source_folder, destination_folder, file, keep_aspect_ratio).run()\n",
    "    \n",
    "image_width = 224\n",
    "keep_aspect_ratio = False\n",
    "quality = 100\n",
    "source_folder = \"ODIR-5K_Testing_Dataset_cropped\"\n",
    "destination_folder = \"ODIR-5K_Testing_Dataset_treated\" + '_' + str(image_width)\n",
    "\n",
    "if os.path.exists(destination_folder):\n",
    "    if len(os.listdir(destination_folder)) < 500:\n",
    "        clear_content(destination_folder)\n",
    "else:\n",
    "    os.mkdir(destination_folder)\n",
    "    \n",
    "resize_all_images(source_folder, destination_folder, image_width, quality, keep_aspect_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-heather",
   "metadata": {},
   "source": [
    "**Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chubby-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "\n",
    "class ImageTreatment:\n",
    "    \"\"\"Used for Augmenting Images\"\"\"\n",
    "    def __init__(self, image_size):\n",
    "        self.image_size = image_size\n",
    "    \n",
    "    def scaling(self, image, scale_vector):\n",
    "        # Resize to 4-D vector\n",
    "        image = np.reshape(image, (1, self.image_size, self.image_size, 3))\n",
    "        boxes = np.zeros((len(scale_vector), 4), dtype=np.float32)\n",
    "        for index, scale in enumerate(scale_vector):\n",
    "            x1 = y1 = 0.5 - 0.5 * scale\n",
    "            x2 = y2 = 0.5 + 0.5 * scale\n",
    "            boxes[index] = np.array([y1, x1, y2, x2], dtype=np.float32)\n",
    "        box_ind = np.zeros((len(scale_vector)), dtype=np.int32)\n",
    "        crop_size = np.array([self.image_size, self.image_size], dtype=np.int32)\n",
    "        \n",
    "        output = tf.image.crop_and_resize(image, boxes, box_ind, crop_size)\n",
    "        output = np.array(output, dtype=np.uint8)\n",
    "        return output\n",
    "    \n",
    "    def brightness(self, image, delta):\n",
    "        output = tf.iamge.adjust_brightness(image, delta)\n",
    "        output = np.array(output, dtype=np.uint8)\n",
    "        return output\n",
    "    \n",
    "    def contrast(self, image, contrast_factor):\n",
    "        output = tf.image.adjust_contrast(image, contrast_factor)\n",
    "        output = np.array(output, dtype=np.uint8)\n",
    "        return output\n",
    "    \n",
    "    def saturation(self, image, saturation_factor):\n",
    "        output = tf.image.adjust_saturation(image, saturation_factor)\n",
    "        output = np.array(output, dtype=np.uint8)\n",
    "        return output\n",
    "    \n",
    "    def hue(self, image, delta):\n",
    "        output = tf.image.adjust_hue(image, delta)\n",
    "        output = np.array(output, dtype=np.uint8)\n",
    "        return output\n",
    "    \n",
    "    def central_crop(self, image, central_fraction):\n",
    "        output = tf.image.central_crop(image, central_fraction)\n",
    "        output = np.array(output, dtype=np.uint8)\n",
    "        return output\n",
    "    \n",
    "    def crop_to_bounding_box(self, image, offset_height, offset_width, target_height, target_width):\n",
    "        output = tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)\n",
    "        output = tf.image.resize(output, (self.image_size, self.image_size))\n",
    "        output = np.array(output, dtype=np.uint8)\n",
    "        return output\n",
    "    \n",
    "    def gamma(self, image, gamma):\n",
    "        output = tf.image.adjust_gamma(image, gamma)\n",
    "        output = np.array(output, dtype=np.uint8)\n",
    "        return output\n",
    "    \n",
    "    def rot90(self, image, k):\n",
    "        output = tf.image.rot90(image, k)\n",
    "        output = np.array(output, dtype=np.uint8)\n",
    "        return output\n",
    "    \n",
    "    def rescale_intensity(self, image):\n",
    "        p2, p98 = np.percentile(image, (2, 98))\n",
    "        img_rescale = exposure.rescale_intensity(image, in_range=(p2, p98))\n",
    "        return img_rescale\n",
    "    \n",
    "    def equalize_histogram(self, image):\n",
    "        img_eq = exposure.equalize_hist(image)\n",
    "        return img_eq\n",
    "    \n",
    "    def equalize_adapthist(self, image):\n",
    "        img_adapted = exposure.equalize_adapthist(image, clipLimit=0.03)\n",
    "        return img_adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "middle-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class DataAugmentationStrategy:\n",
    "    \"\"\"Augmentation Strategy - Manual Aug\"\"\"\n",
    "    def __init__(self, image_size, file_name):\n",
    "        self.base_image = file_name\n",
    "        self.treatment = ImageTreatment(image_size)\n",
    "        self.file_path = \"ODIR-5k_Training_Dataset_treated_\" + str(image_size)\n",
    "        self.saving_path = \"ODIR-5k_Training_Dataset_augmented_\" + str(image_size)\n",
    "        self.file_id = file_name.replace('.jpg', '')\n",
    "        \n",
    "        \n",
    "    def save_image(self, original_vector, image, sample):\n",
    "        central = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        file = self.file_id + '_' + str(sample) + '.jpg'\n",
    "        \n",
    "\n",
    "        file_name = os.path.join(self.saving_path, file)\n",
    "        exists = os.path.isfile(file_name)\n",
    "        if exists:\n",
    "            print('duplicate file found: ' + file_name)\n",
    "            \n",
    "        status = cv2.imwrite(file_name, central)\n",
    "        \n",
    "        if os.path.exists('ground_truth'):\n",
    "            if len(os.listdir('ground_truth')) < 1:\n",
    "                clear_content('groud_truth')\n",
    "        else:\n",
    "            os.mkdir('ground_truth')\n",
    "            \n",
    "        with open(r'ground_truth/odir_augmented.csv', 'a', newline='') as csv_file:\n",
    "            file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            file_writer.writerow([file, original_vector[1], original_vector[2], original_vector[3], original_vector[4],\n",
    "                                 original_vector[5], original_vector[6], original_vector[7], original_vector[8]])\n",
    "        \n",
    "    def generate_images(self, number_samples, original_vector, weights):\n",
    "        eye_image = os.path.join(self.file_path, self.base_image)\n",
    "        image = cv2.imread(eye_image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        original_image = image\n",
    "        saved = 0\n",
    "        \n",
    "        # For any repeating elements, just give the other output\n",
    "        # We are only expecting up to 3 repetitions\n",
    "        if weights == 20:\n",
    "            original_image = self.treatment.rot90(original_image, 2)\n",
    "        if weights == 400:\n",
    "            original_image = self.treatment.rot90(original_image, 3)\n",
    "        if weights > 401:\n",
    "            print(str(self.file_id) + ' samples:' + str(number_samples))\n",
    "            raise ValueError('this cannot happen')\n",
    "\n",
    "        # for the sample type 14, just generate 1 image and leave the method\n",
    "        if number_samples == 14:\n",
    "            central = self.treatment.rot90(original_image, 1)\n",
    "            self.save_image(original_vector, central, weights+14)\n",
    "            saved = saved +1\n",
    "            return saved\n",
    "\n",
    "        if number_samples > 0:\n",
    "            central = self.treatment.crop_to_bounding_box(original_image, 0, 0, 112, 112)\n",
    "            self.save_image(original_vector, central, weights+0)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 1:\n",
    "            central = self.treatment.crop_to_bounding_box(original_image, 112, 0, 112, 112)\n",
    "            self.save_image(original_vector, central, weights+1)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 2:\n",
    "            central = self.treatment.crop_to_bounding_box(original_image, 0, 112, 112, 112)\n",
    "            self.save_image(original_vector, central, weights+2)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 3:\n",
    "            central = self.treatment.crop_to_bounding_box(original_image, 112, 112, 112, 112)\n",
    "            self.save_image(original_vector, central, weights+3)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 4:\n",
    "            vector = [0.50]\n",
    "            central = self.treatment.scaling(original_image, vector)\n",
    "            self.save_image(original_vector, central[0], weights+4)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 5:\n",
    "            vector = [0.70]\n",
    "            central = self.treatment.scaling(original_image, vector)\n",
    "            self.save_image(original_vector, central[0], weights+5)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 6:\n",
    "            vector = [0.80]\n",
    "            central = self.treatment.scaling(original_image, vector)\n",
    "            self.save_image(original_vector, central[0], weights+6)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 7:\n",
    "            vector = [0.90]\n",
    "            central = self.treatment.scaling(original_image, vector)\n",
    "            self.save_image(original_vector, central[0], weights+7)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 8:\n",
    "            central = self.treatment.rescale_intensity(original_image)\n",
    "            self.save_image(original_vector, central, weights+8)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 9:\n",
    "            central = self.treatment.contrast(original_image, 2)\n",
    "            self.save_image(original_vector, central, weights+9)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 10:\n",
    "            central = self.treatment.saturation(original_image, 0.5)\n",
    "            self.save_image(original_vector, central, weights+10)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 11:\n",
    "            central = self.treatment.gamma(original_image, 0.5)\n",
    "            self.save_image(original_vector, central, weights+11)\n",
    "            saved = saved + 1\n",
    "\n",
    "        if number_samples > 12:\n",
    "            central = self.treatment.hue(original_image, 0.2)\n",
    "            self.save_image(original_vector, central, weights+12)\n",
    "            saved = saved + 1\n",
    "\n",
    "        return saved\n",
    "\n",
    "class GroundTruthFiles:\n",
    "    def __init__(self):\n",
    "        self.amd = []\n",
    "        self.cataract = []\n",
    "        self.diabetes = []\n",
    "        self.glaucoma = []\n",
    "        self.hypertension = []\n",
    "        self.myopia = []\n",
    "        self.others = []\n",
    "\n",
    "    def populate_vectors(self, ground_truth_file):\n",
    "        with open(ground_truth_file) as csvDataFile:\n",
    "            csv_reader = csv.reader(csvDataFile)\n",
    "\n",
    "            for row in csv_reader:\n",
    "                column_id = row[0]\n",
    "                normal = row[1]\n",
    "                diabetes = row[2]\n",
    "                glaucoma = row[3]\n",
    "                cataract = row[4]\n",
    "                amd = row[5]\n",
    "                hypertension = row[6]\n",
    "                myopia = row[7]\n",
    "                others = row[8]\n",
    "                # just discard the first row\n",
    "                if column_id != \"ID\":\n",
    "#                     print(\"Processing image: \" + column_id.split('_')[0] + \"_left.jpg\")\n",
    "                    if diabetes == '1':\n",
    "                        self.diabetes.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                    if glaucoma == '1':\n",
    "                        self.glaucoma.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                    if cataract == '1':\n",
    "                        self.cataract.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                    if amd == '1':\n",
    "                        self.amd.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                    if hypertension == '1':\n",
    "                        self.hypertension.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                    if myopia == '1':\n",
    "                        self.myopia.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                    if others == '1':\n",
    "                        self.others.append([column_id, normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "juvenile-adjustment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files record count order by size ASC\n",
      "hypertension 192\n",
      "myopia 262\n",
      "cataract 275\n",
      "amd 280\n",
      "glaucoma 313\n",
      "others 1124\n",
      "diabetes 1778\n",
      "total generated hypertension: 2624\n",
      "total generated myopia: 2554\n",
      "total generated cataract: 2541\n",
      "total generated amd: 2536\n",
      "total generated glaucoma: 2503\n",
      "total generated others: 1692\n",
      "total generated diabetes: 1038\n"
     ]
    }
   ],
   "source": [
    "# run augmentation strategy\n",
    "from absl import app\n",
    "import csv\n",
    "\n",
    "def write_header():\n",
    "    with open(r'ground_truth\\odir_augmented.csv', 'w', newline='') as csv_file:\n",
    "        file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        file_writer.writerow(['ID', 'Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension',\n",
    "                              'Myopia', 'Others'])\n",
    "        return file_writer\n",
    "\n",
    "\n",
    "def process_files(images, cache, files):\n",
    "    total = 0\n",
    "    for strategy in range(len(images)):\n",
    "        images_to_process = images[strategy][0]\n",
    "        samples_per_image = images[strategy][1]\n",
    "        for image_index in range(images_to_process):\n",
    "            image_vector = files[image_index]\n",
    "            file_name = image_vector[0]\n",
    "\n",
    "            # Only check during the first strategy\n",
    "            if strategy == 0:\n",
    "                if file_name not in cache:\n",
    "                    cache[file_name] = 1\n",
    "                else:\n",
    "                    cache[file_name] = cache[file_name] * 20\n",
    "\n",
    "            # print('Processing: ' + file_name)\n",
    "            augment = DataAugmentationStrategy(image_size, file_name)\n",
    "            count = augment.generate_images(samples_per_image, image_vector, cache[file_name])\n",
    "            total = total + count\n",
    "    return total\n",
    "\n",
    "\n",
    "def main(csv_path):\n",
    "    # load the ground truth file\n",
    "    files = GroundTruthFiles()\n",
    "    files.populate_vectors(csv_path)\n",
    "\n",
    "    print('files record count order by size ASC')\n",
    "    print('hypertension ' + str(len(files.hypertension)))\n",
    "    print('myopia ' + str(len(files.myopia)))\n",
    "    print('cataract ' + str(len(files.cataract)))\n",
    "    print('amd ' + str(len(files.amd)))\n",
    "    print('glaucoma ' + str(len(files.glaucoma)))\n",
    "    print('others ' + str(len(files.others)))\n",
    "    print('diabetes ' + str(len(files.diabetes)))\n",
    "\n",
    "    images_hypertension = [[len(files.hypertension), 13], [128, 14]]\n",
    "    images_myopia = [[len(files.myopia), 9], [196, 14]]\n",
    "    images_cataract = [[len(files.cataract), 9], [66, 14]]\n",
    "    images_amd = [[len(files.amd), 9], [16, 14]]\n",
    "    images_glaucoma = [[len(files.glaucoma), 7], [312, 14]]\n",
    "    images_others = [[len(files.others), 1], [568, 14]]\n",
    "    images_diabetes = [[1038, 1]]\n",
    "\n",
    "    # Delete previous file\n",
    "    exists = os.path.isfile(r'ground_truth\\odir_augmented.csv')\n",
    "    if exists:\n",
    "        os.remove(r'ground_truth\\odir_augmented.csv')\n",
    "\n",
    "    write_header()\n",
    "\n",
    "    images_processed = {}\n",
    "\n",
    "    total_hypertension = process_files(images_hypertension, images_processed, files.hypertension)\n",
    "    total_myopia = process_files(images_myopia, images_processed, files.myopia)\n",
    "    total_cataract = process_files(images_cataract, images_processed, files.cataract)\n",
    "    total_amd = process_files(images_amd, images_processed, files.amd)\n",
    "    total_glaucoma = process_files(images_glaucoma, images_processed, files.glaucoma)\n",
    "    total_others = process_files(images_others, images_processed, files.others)\n",
    "    total_diabetes = process_files(images_diabetes, images_processed, files.diabetes)\n",
    "\n",
    "    print(\"total generated hypertension: \" + str(total_hypertension))\n",
    "    print(\"total generated myopia: \" + str(total_myopia))\n",
    "    print(\"total generated cataract: \" + str(total_cataract))\n",
    "    print(\"total generated amd: \" + str(total_amd))\n",
    "    print(\"total generated glaucoma: \" + str(total_glaucoma))\n",
    "    print(\"total generated others: \" + str(total_others))\n",
    "    print(\"total generated diabetes: \" + str(total_diabetes))\n",
    "\n",
    "image_size = 224\n",
    "save_path = \"ODIR-5k_Training_Dataset_augmented_\" + str(image_size)\n",
    "\n",
    "csv_path = 'ground_truth/odir.csv'\n",
    "main(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "flush-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# odir_patients_to_numpy.py\n",
    "\n",
    "from absl import app\n",
    "import logging\n",
    "import logging.config\n",
    "import time\n",
    "import csv\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "class NumpyDataGenerator:\n",
    "    def __init__(self, training_path, testing_path, csv_path, csv_testing_path, augmented_path, csv_augmented_file):\n",
    "        self.training_path = training_path\n",
    "        self.testing_path = testing_path\n",
    "        self.csv_path = csv_path\n",
    "        self.csv_testing_path = csv_testing_path\n",
    "#         self.logger = logging.getLogger('odir')\n",
    "        self.total_records_training = 0\n",
    "        self.total_records_testing = 0\n",
    "        self.csv_augmented_path = csv_augmented_file\n",
    "        self.augmented_path = augmented_path\n",
    "\n",
    "    def npy_training_files(self, file_name_training, file_name_training_labels):\n",
    "        training = []\n",
    "        training_labels = []\n",
    "\n",
    "#         self.logger.debug(\"Opening CSV file\")\n",
    "        with open(self.csv_path) as csvDataFile:\n",
    "            csv_reader = csv.reader(csvDataFile)\n",
    "            self.total_records_training = 0\n",
    "            for row in csv_reader:\n",
    "                column_id = row[0]\n",
    "                normal = row[1]\n",
    "                diabetes = row[2]\n",
    "                glaucoma = row[3]\n",
    "                cataract = row[4]\n",
    "                amd = row[5]\n",
    "                hypertension = row[6]\n",
    "                myopia = row[7]\n",
    "                others = row[8]\n",
    "                # just discard the first row\n",
    "                if column_id != \"ID\":\n",
    "#                     self.logger.debug(\"Processing image: \" + column_id)\n",
    "                    # load first the image from the folder\n",
    "                    eye_image = os.path.join(self.training_path, column_id)\n",
    "                    image = cv2.imread(eye_image)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    training.append(image)\n",
    "                    training_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                    self.total_records_training = self.total_records_training + 1\n",
    "\n",
    "        training = np.array(training, dtype='uint8')\n",
    "        training_labels = np.array(training_labels, dtype='uint8')\n",
    "        # convert (number of images x height x width x number of channels) to (number of images x (height * width *3))\n",
    "        # for example (6069 * 28 * 28 * 3)-> (6069 x 2352) (14,274,288)\n",
    "        training = np.reshape(training, [training.shape[0], training.shape[1], training.shape[2], training.shape[3]])\n",
    "\n",
    "        # save numpy array as .npy formats\n",
    "        np.save(file_name_training, training)\n",
    "#         self.logger.debug(\"Saving NPY File: \" + file_name_training)\n",
    "        np.save(file_name_training_labels, training_labels)\n",
    "#         self.logger.debug(\"Saving NPY File: \" + file_name_training_labels)\n",
    "#         self.logger.debug(\"Closing CSV file\")\n",
    "\n",
    "    def npy_testing_files(self, file_name_testing, file_name_testing_labels):\n",
    "        testing = []\n",
    "        testing_labels = []\n",
    "\n",
    "#         self.logger.debug(\"Opening CSV file\")\n",
    "        with open(self.csv_testing_path) as csvDataFile:\n",
    "            csv_reader = csv.reader(csvDataFile)\n",
    "            self.total_records_testing = 0\n",
    "            for row in csv_reader:\n",
    "                column_id = row[0]\n",
    "                normal = row[1]\n",
    "                diabetes = row[2]\n",
    "                glaucoma = row[3]\n",
    "                cataract = row[4]\n",
    "                amd = row[5]\n",
    "                hypertension = row[6]\n",
    "                myopia = row[7]\n",
    "                others = row[8]\n",
    "                # just discard the first row\n",
    "                if column_id != \"ID\":\n",
    "#                     self.logger.debug(\"Processing image: \" + column_id + \"_left.jpg\")\n",
    "                    # load first the image from the folder\n",
    "                    eye_image = os.path.join(self.testing_path, column_id + \"_left.jpg\")\n",
    "                    image = cv2.imread(eye_image)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    testing.append(image)\n",
    "                    testing_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                    self.total_records_testing = self.total_records_testing + 1\n",
    "\n",
    "#                     self.logger.debug(\"Processing image: \" + column_id + \"_right.jpg\")\n",
    "                    eye_image = os.path.join(self.testing_path, column_id + \"_right.jpg\")\n",
    "                    image = cv2.imread(eye_image)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    testing.append(image)\n",
    "                    testing_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                    self.total_records_testing = self.total_records_testing + 1\n",
    "\n",
    "        testing = np.array(testing, dtype='uint8')\n",
    "        training_labels = np.array(testing_labels, dtype='uint8')\n",
    "        # convert (number of images x height x width x number of channels) to (number of images x (height * width *3))\n",
    "        # for example (6069 * 28 * 28 * 3)-> (6069 x 2352) (14,274,288)\n",
    "        testing = np.reshape(testing, [testing.shape[0], testing.shape[1], testing.shape[2], testing.shape[3]])\n",
    "\n",
    "        # save numpy array as .npy formats\n",
    "        np.save(file_name_testing, testing)\n",
    "#         self.logger.debug(\"Saving NPY File: \" + file_name_testing)\n",
    "        np.save(file_name_testing_labels, training_labels)\n",
    "#         self.logger.debug(\"Saving NPY File: \" + file_name_testing_labels)\n",
    "#         self.logger.debug(\"Closing CSV file\")\n",
    "\n",
    "    def npy_training_files_split(self, split_number, file_name_training, file_name_training_labels, file_name_testing,\n",
    "                                 file_name_testing_labels):\n",
    "        training = []\n",
    "        training_labels = []\n",
    "        testing = []\n",
    "        testing_labels = []\n",
    "\n",
    "#         self.logger.debug(\"Opening CSV file\")\n",
    "        count = 0\n",
    "        with open(self.csv_path) as csvDataFile:\n",
    "            csv_reader = csv.reader(csvDataFile)\n",
    "            self.total_records_training = 0\n",
    "            self.total_records_testing = 0\n",
    "            for row in csv_reader:\n",
    "                column_id = row[0]\n",
    "                label = row[1]\n",
    "                # just discard the first row\n",
    "                if column_id != \"ID\":\n",
    "#                     self.logger.debug(\"Processing image: \" + column_id)\n",
    "                    # load first the image from the folder\n",
    "                    eye_image = os.path.join(self.training_path, column_id)\n",
    "                    image = cv2.imread(eye_image)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    if count < split_number:\n",
    "                        testing.append(image)\n",
    "                        testing_labels.append(label)\n",
    "                        self.total_records_testing = self.total_records_testing + 1\n",
    "                    else:\n",
    "                        training.append(image)\n",
    "                        training_labels.append(label)\n",
    "                        self.total_records_training = self.total_records_training + 1\n",
    "                    count = count + 1\n",
    "\n",
    "        testing = np.array(testing, dtype='uint8')\n",
    "        testing_labels = np.array(testing_labels, dtype='uint8')\n",
    "        testing = np.reshape(testing, [testing.shape[0], testing.shape[1], testing.shape[2], testing.shape[3]])\n",
    "\n",
    "        # save numpy array as .npy formats\n",
    "        np.save(file_name_testing, testing)\n",
    "        np.save(file_name_testing_labels, testing_labels)\n",
    "\n",
    "        training = np.array(training, dtype='uint8')\n",
    "        training_labels = np.array(training_labels, dtype='uint8')\n",
    "        # convert (number of images x height x width x number of channels) to (number of images x (height * width *3))\n",
    "        # for example (6069 * 28 * 28 * 3)-> (6069 x 2352) (14,274,288)\n",
    "        training = np.reshape(training, [training.shape[0], training.shape[1], training.shape[2], training.shape[3]])\n",
    "\n",
    "        # save numpy array as .npy formats\n",
    "        np.save(file_name_training, training)\n",
    "#         self.logger.debug(\"Saving NPY File: \" + file_name_training)\n",
    "        np.save(file_name_training_labels, training_labels)\n",
    "#         self.logger.debug(\"Saving NPY File: \" + file_name_training_labels)\n",
    "#         self.logger.debug(\"Closing CSV file\")\n",
    "\n",
    "    def is_sickness(self, row, sickness):\n",
    "        switcher = {\n",
    "            \"normal\": row[1] == '1' and row[2] == '0' and row[3] == '0' and row[4] == '0' and row[5] == '0' and row[\n",
    "                6] == '0' and row[7] == '0' and row[8] == '0',\n",
    "            \"diabetes\": row[1] == '0' and row[2] == '1' and row[3] == '0' and row[4] == '0' and row[5] == '0' and row[\n",
    "                6] == '0' and row[7] == '0' and row[8] == '0',\n",
    "            \"glaucoma\": row[1] == '0' and row[2] == '0' and row[3] == '1' and row[4] == '0' and row[5] == '0' and row[\n",
    "                6] == '0' and row[7] == '0' and row[8] == '0',\n",
    "            \"cataract\": row[1] == '0' and row[2] == '0' and row[3] == '0' and row[4] == '1' and row[5] == '0' and row[\n",
    "                6] == '0' and row[7] == '0' and row[8] == '0',\n",
    "            \"amd\": row[1] == '0' and row[2] == '0' and row[3] == '0' and row[4] == '0' and row[5] == '1' and row[\n",
    "                6] == '0' and row[7] == '0' and row[8] == '0',\n",
    "            \"hypertension\": row[1] == '0' and row[2] == '0' and row[3] == '0' and row[4] == '0' and row[5] == '0' and\n",
    "                            row[6] == '1' and row[7] == '0' and row[8] == '0',\n",
    "            \"myopia\": row[1] == '0' and row[2] == '0' and row[3] == '0' and row[4] == '0' and row[5] == '0' and row[\n",
    "                6] == '0' and row[7] == '1' and row[8] == '0',\n",
    "            \"others\": row[1] == '0' and row[2] == '0' and row[3] == '0' and row[4] == '0' and row[5] == '0' and row[\n",
    "                6] == '0' and row[7] == '0' and row[8] == '1'\n",
    "        }\n",
    "        return switcher.get(sickness, False)\n",
    "\n",
    "    def npy_training_files_split_all(self, split_number, file_name_training, file_name_training_labels,\n",
    "                                     file_name_testing,\n",
    "                                     file_name_testing_labels, include_augmented):\n",
    "        split_factor = 10820\n",
    "        training = []\n",
    "        training_labels = []\n",
    "        training_2 = []\n",
    "        training_labels_2 = []\n",
    "        testing = []\n",
    "        testing_labels = []\n",
    "        images_used = []\n",
    "        count_images = 0\n",
    "\n",
    "        class_names = ['normal', 'diabetes', 'glaucoma', 'cataract', 'amd',\n",
    "                       'hypertension', 'myopia', 'others']\n",
    "\n",
    "#         self.logger.debug(\"Opening CSV file\")\n",
    "\n",
    "        class_count = {'normal': 0, 'diabetes': 0, 'glaucoma': 0, 'cataract': 0, 'amd': 0, 'hypertension': 0,\n",
    "                       'myopia': 0, 'others': 0}\n",
    "        split_pocket = split_number / 8\n",
    "        with open(self.csv_path) as csvDataFile:\n",
    "            csv_reader = csv.reader(csvDataFile)\n",
    "            self.total_records_training = 0\n",
    "            self.total_records_testing = 0\n",
    "            for row in csv_reader:\n",
    "                column_id = row[0]\n",
    "                normal = row[1]\n",
    "                diabetes = row[2]\n",
    "                glaucoma = row[3]\n",
    "                cataract = row[4]\n",
    "                amd = row[5]\n",
    "                hypertension = row[6]\n",
    "                myopia = row[7]\n",
    "                others = row[8]\n",
    "                # just discard the first row\n",
    "                if column_id != \"ID\":\n",
    "#                     self.logger.debug(\"Processing image: \" + column_id)\n",
    "                    # load first the image from the folder\n",
    "                    eye_image = os.path.join(self.training_path, column_id)\n",
    "                    image = cv2.imread(eye_image)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    found = False\n",
    "                    for sickness in class_names:\n",
    "                        if self.is_sickness(row, sickness) and class_count[sickness] < split_pocket:\n",
    "                            testing.append(image)\n",
    "                            images_used.append(row[0] + ',' + sickness + ',' + str(class_count[sickness]))\n",
    "                            testing_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                            self.total_records_testing = self.total_records_testing + 1\n",
    "                            class_count[sickness] = class_count[sickness] + 1\n",
    "                            found = True\n",
    "#                             logger.debug('found ' + sickness + ' ' + str(class_count[sickness]))\n",
    "\n",
    "                    if not found:\n",
    "                        training.append(image)\n",
    "                        training_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                        self.total_records_training = self.total_records_training + 1\n",
    "                        count_images = count_images + 1\n",
    "\n",
    "        if include_augmented:\n",
    "            with open(self.csv_augmented_path) as csvDataFile:\n",
    "                csv_reader = csv.reader(csvDataFile)\n",
    "                for row in csv_reader:\n",
    "                    column_id = row[0]\n",
    "                    normal = row[1]\n",
    "                    diabetes = row[2]\n",
    "                    glaucoma = row[3]\n",
    "                    cataract = row[4]\n",
    "                    amd = row[5]\n",
    "                    hypertension = row[6]\n",
    "                    myopia = row[7]\n",
    "                    others = row[8]\n",
    "                    # just discard the first row\n",
    "                    if column_id != \"ID\":\n",
    "#                         self.logger.debug(\"Processing image: \" + column_id)\n",
    "                        # load first the image from the folder\n",
    "                        eye_image = os.path.join(self.augmented_path, column_id)\n",
    "                        image = cv2.imread(eye_image)\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                        if count_images >= split_factor:\n",
    "                            training_2.append(image)\n",
    "                            training_labels_2.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                        else:\n",
    "                            training.append(image)\n",
    "                            training_labels.append([normal, diabetes, glaucoma, cataract, amd, hypertension, myopia, others])\n",
    "                        self.total_records_training = self.total_records_training + 1\n",
    "                        count_images = count_images + 1\n",
    "\n",
    "        testing = np.array(testing, dtype='uint8')\n",
    "        testing_labels = np.array(testing_labels, dtype='uint8')\n",
    "        testing = np.reshape(testing, [testing.shape[0], testing.shape[1], testing.shape[2], testing.shape[3]])\n",
    "\n",
    "        # save numpy array as .npy formats\n",
    "        np.save(file_name_testing, testing)\n",
    "        np.save(file_name_testing_labels, testing_labels)\n",
    "\n",
    "        training = np.array(training, dtype='uint8')\n",
    "        training_labels = np.array(training_labels, dtype='uint8')\n",
    "        # convert (number of images x height x width x number of channels) to (number of images x (height * width *3))\n",
    "        # for example (6069 * 28 * 28 * 3)-> (6069 x 2352) (14,274,288)\n",
    "        training = np.reshape(training, [training.shape[0], training.shape[1], training.shape[2], training.shape[3]])\n",
    "\n",
    "        # convert (number of images x height x width x number of channels) to (number of images x (height * width *3))\n",
    "        # for example (6069 * 28 * 28 * 3)-> (6069 x 2352) (14,274,288)\n",
    "        if include_augmented:\n",
    "            training_2 = np.array(training_2, dtype='uint8')\n",
    "            training_labels_2 = np.array(training_labels_2, dtype='uint8')\n",
    "            training_2 = np.reshape(training_2, [training_2.shape[0], training_2.shape[1], training_2.shape[2], training_2.shape[3]])\n",
    "\n",
    "#         self.logger.debug(testing.shape)\n",
    "#         self.logger.debug(testing_labels.shape)\n",
    "#         self.logger.debug(training.shape)\n",
    "#         self.logger.debug(training_labels.shape)\n",
    "#         if include_augmented:\n",
    "#             self.logger.debug(training_2.shape)\n",
    "#             self.logger.debug(training_labels_2.shape)\n",
    "\n",
    "        # save numpy array as .npy formats\n",
    "        np.save(file_name_training + '_1', training)\n",
    "        np.save(file_name_training_labels + '_1', training_labels)\n",
    "        if include_augmented:\n",
    "            np.save(file_name_training + '_2', training_2)\n",
    "            np.save(file_name_training_labels + '_2', training_labels_2)\n",
    "#         self.logger.debug(\"Closing CSV file\")\n",
    "#         for sickness in class_names:\n",
    "#             self.logger.debug('found ' + sickness + ' ' + str(class_count[sickness]))\n",
    "        csv_writer = csv.writer(open(\"files_used.csv\", 'w', newline=''))\n",
    "        for item in images_used:\n",
    "#             self.logger.debug(item)\n",
    "            entries = item.split(\",\")\n",
    "            csv_writer.writerow(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "civic-charles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    start = time.time()\n",
    "    image_width = 224\n",
    "    training_path = r'ODIR-5K_Training_Dataset_treated' + '_' + str(image_width)\n",
    "    testing_path = r'ODIR-5K_Testing_Dataset_treated' + '_' + str(image_width)\n",
    "    augmented_path = r'ODIR-5K_Training_Dataset_augmented' + '_' + str(image_width)\n",
    "    csv_file = r'ground_truth\\odir.csv'\n",
    "    csv_augmented_file = r'ground_truth\\odir_augmented.csv'\n",
    "    training_file = r'ground_truth\\testing_default_value.csv'\n",
    "    generator = NumpyDataGenerator(training_path, testing_path, csv_file, training_file, augmented_path,\n",
    "                                   csv_augmented_file)\n",
    "\n",
    "    # Generate testing file\n",
    "    generator.npy_testing_files('odir_testing_challenge' + '_' + str(image_width), 'odir_testing_labels_challenge' + '_' + str(image_width))\n",
    "\n",
    "    # Generate training file\n",
    "    generator.npy_training_files('odir_training', 'odir_training_labels')\n",
    "    generator.npy_training_files_split(1000, 'odir_training',\n",
    "    'odir_training_labels', 'odir_testing', 'odir_testing_labels')\n",
    "\n",
    "    generator.npy_training_files_split_all(400, 'odir_training' + '_' + str(image_width),\n",
    "                                           'odir_training_labels' + '_' + str(image_width),\n",
    "                                           'odir_testing' + '_' + str(image_width),\n",
    "                                           'odir_testing_labels' + '_' + str(image_width),\n",
    "                                           True)\n",
    "    end = time.time()\n",
    "    \n",
    "main()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-catering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOFT_ENG2",
   "language": "python",
   "name": "soft_eng2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
