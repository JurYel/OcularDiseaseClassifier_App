{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fantastic-announcement",
   "metadata": {},
   "source": [
    "### Find Contours of Retinal Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "described-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-trance",
   "metadata": {},
   "source": [
    "#### Method #1 \n",
    "* Method seems to result into disoriented retinal images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-lover",
   "metadata": {},
   "source": [
    "#### Remove Black Pixels on Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "parental-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../../assets/datasets/odir5k/ODIR-5k/ODIR-5k/Training Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "earned-progress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) c:\\users\\appveyor\\appdata\\local\\temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,-1,-1>,struct cv::impl::A0x3a7f3a8f::Set<3,4,-1>,struct cv::impl::A0x3a7f3a8f::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-d7b13df34c38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcropped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[0mnew_image1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrop_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[0mnew_image2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrop_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'u'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[0mnew_image3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrop_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'v'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-d7b13df34c38>\u001b[0m in \u001b[0;36mcrop_image\u001b[1;34m(image, plot, image_size, channel)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# convert back to bgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mnew_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_YUV2BGR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mnew_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) c:\\users\\appveyor\\appdata\\local\\temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,-1,-1>,struct cv::impl::A0x3a7f3a8f::Set<3,4,-1>,struct cv::impl::A0x3a7f3a8f::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": [
    "# abs_path = \"preprocess_train\" # not work cause already preprocessed\n",
    "train_files = sorted(os.listdir(train_path))\n",
    "rand_idx = np.random.randint(0, len(train_files))\n",
    "image = cv2.imread(os.path.join(abs_path, train_files[rand_idx]))\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "# image = cv2.resize(image, None, fx=0.3, fy=0.3, interpolation=cv2.INTER_AREA)\n",
    "cv2.imshow(\"Original\", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "def crop_image(image, plot=False, image_size=(256, 256), channel='y'):\n",
    "    # mask of colored pixels\n",
    "    mask = image > 0\n",
    "\n",
    "    # coordinates of colored pixels\n",
    "    coordinates = np.argwhere(mask)\n",
    "\n",
    "    # binding box of non-black pixels.\n",
    "    x0, y0, s0 = coordinates.min(axis=0)\n",
    "    x1, y1, s1 = coordinates.max(axis=0) + 1 # slices are exclusive at the top\n",
    "\n",
    "    # get the contents of the bounding box\n",
    "    cropped = image[x0:x1, y0:y1]\n",
    "    \n",
    "    # convert to YUV for equalization\n",
    "    img_yuv = cv2.cvtColor(cropped, cv2.COLOR_BGR2YUV)\n",
    "    \n",
    "    # apply adaptive equaliation on Y\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    if channel == 'y':\n",
    "        img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])\n",
    "        \n",
    "        # apply image normalization \n",
    "        mask = np.zeros((256, 256))\n",
    "        normalized = cv2.normalize(img_yuv[:, :, 0], mask, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    elif channel == 'u':\n",
    "        img_yuv[:, 0, :] = clahe.apply(img_yuv[:, 0, :])\n",
    "        \n",
    "        # apply image normalization \n",
    "        mask = np.zeros((256, 256))\n",
    "        normalized = cv2.normalize(img_yuv[:, 0, :], mask, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    elif channel == 'v':\n",
    "        img_yuv[0, :, :] = clahe.apply(img_yuv[0, :, :])\n",
    "        \n",
    "        # apply image normalization \n",
    "        mask = np.zeros((256, 256))\n",
    "        normalized = cv2.normalize(img_yuv[0, :, :], mask, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    \n",
    "    # apply image normalization \n",
    "#     mask = np.zeros((256, 256))\n",
    "#     normalized = cv2.normalize(img_yuv[:, :, 0], mask, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # convert back to bgr\n",
    "    new_image = cv2.cvtColor(normalized, cv2.COLOR_YUV2BGR)\n",
    "    new_image = cv2.resize(new_image, image_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    if plot:\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Original - {image.shape}\")\n",
    "        plt.axis('off')\n",
    "        plt.grid(False)\n",
    "        \n",
    "        # ----------------- #\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(new_image)\n",
    "        plt.title(f\"Preprocessed - {new_image.shape}\")\n",
    "        plt.axis('off')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "        \n",
    "    return cropped\n",
    "\n",
    "new_image1 = crop_image(image, plot=False, channel='y')\n",
    "new_image2 = crop_image(image, plot=False, channel='u')\n",
    "new_image3 = crop_image(image, plot=False, channel='v')\n",
    "print(new_image1.shape)\n",
    "\n",
    "cv2.imshow(\"CLAHE at Y\", new_image1)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.imshow(\"CLAHE at U\", new_image2)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.imshow(\"CLAHE at V\", new_image3)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# overwrite the same file\n",
    "# cv2.imshow(\"cropped\", crop_image(image))\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unsigned-charm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5608"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-tunnel",
   "metadata": {},
   "source": [
    "**Updated Preprocessing Methods**\n",
    "* added adaptive histogram equalization\n",
    "* added image normalization\n",
    "* performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "accompanied-gregory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(369, 486)\n",
      "(337, 256)\n"
     ]
    }
   ],
   "source": [
    "def keep_aspect_ratio(image):\n",
    "    width_percentage = (256 / float(image.shape[0]))\n",
    "    height_size = int((float(image.shape[1]) * (float(width_percentage))))\n",
    "    cropped = cv2.resize(image, (256, height_size), interpolation=cv2.INTER_AREA)\n",
    "    return cropped\n",
    "\n",
    "image = cv2.imread(\"../../assets/images/3190_left.jpg\")\n",
    "image = cv2.resize(image, None, fx=0.3, fy=0.3 , interpolation=cv2.INTER_AREA)\n",
    "cv2.imshow(\"Original\", image)\n",
    "cv2.waitKey(0)\n",
    "# grayscale and blur to reduce noise\n",
    "grayed = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "grayed = cv2.GaussianBlur(grayed, (5,5), 0)\n",
    "\n",
    "# binarize image for erosion and dilation\n",
    "# erosion removes the residual white noise and shrinks\n",
    "# dilation enlarges the image since it has been noise-reduced\n",
    "# threshold_image = cv2.threshold(grayed, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "threshold_image = cv2.erode(grayed, None, iterations=2)\n",
    "threshold_image = cv2.dilate(threshold_image, None, iterations=2)\n",
    "\n",
    "# find contours\n",
    "contours = cv2.findContours(threshold_image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = imutils.grab_contours(contours)\n",
    "c = max(contours, key=cv2.contourArea)\n",
    "\n",
    "# extract bounding coords\n",
    "extreme_pnts_left = tuple(c[c[:, :, 0].argmin()][0])\n",
    "extreme_pnts_right = tuple(c[c[:, :, 0].argmax()][0])\n",
    "extreme_pnts_top = tuple(c[c[:, :, 1].argmin()][0])\n",
    "extreme_pnts_bot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "# crop and resize image to (256, 256)\n",
    "new_image = grayed[extreme_pnts_top[1]:extreme_pnts_bot[1], extreme_pnts_left[0]:extreme_pnts_right[0]]\n",
    "# new_image = cv2.equalizeHist(new_image) # boosted noise\n",
    "clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "equalized = clahe.apply(new_image)\n",
    "# new_image = cv2.resize(new_image, (256, 256), interpolation = cv2.INTER_AREA)\n",
    "normalizedImg = np.zeros((256, 256))\n",
    "normalizedImg = cv2.normalize(equalized, normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
    "cropped = cv2.resize(normalizedImg, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "cropped = cv2.cvtColor(cropped, cv2.COLOR_GRAY2BGR)\n",
    "print(normalizedImg.shape)\n",
    "# keep aspect ratio\n",
    "# cropped = keep_aspect_ratio(normalizedImg)\n",
    "print(cropped.shape)\n",
    "cv2.imshow(\"Cropped without Adaptive HE\", new_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow(\"Cropped with Adaptive HE\", equalized)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow(\"Equalized and Normalized\", cropped)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-bidding",
   "metadata": {},
   "source": [
    "#### Method #2\n",
    "* Performs better than prior method but still results in fewer disoriented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "driving-sending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COntours Found:  2\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"../../assets/images/3080_right.jpg\")\n",
    "image = cv2.resize(image, None, fx=0.3, fy=0.3, interpolation=cv2.INTER_AREA)\n",
    "test = image.copy()\n",
    "\n",
    "cv2.imshow(\"Original\", test)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# gray and gaussian blur to reduce salt and pepper\n",
    "test = cv2.cvtColor(test, cv2.COLOR_BGR2GRAY)\n",
    "test = cv2.GaussianBlur(test, (5,5), 0)\n",
    "\n",
    "# erode and dilate \n",
    "eroded = cv2.erode(test, None, iterations=2)\n",
    "eroded = cv2.dilate(test, None, iterations=2)\n",
    "\n",
    "cv2.imshow(\"Eroded and Dilated\", eroded)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Find the Canny edges\n",
    "# to better find the external contours of the image\n",
    "edges = cv2.Canny(eroded, 20, 70)\n",
    "\n",
    "# denoise the image\n",
    "edges = cv2.fastNlMeansDenoising(edges, None, 7, 7, 21)\n",
    "cv2.imshow(\"Canny Edges\", edges)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# find the contours\n",
    "contours = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(\"COntours Found: \", len(contours))\n",
    "cv2.imshow(\"Canny Edges After Contouring\", edges)\n",
    "cv2.waitKey(0)\n",
    "contours = imutils.grab_contours(contours)\n",
    "\n",
    "# draw contours\n",
    "# -1 draws all contours\n",
    "# 0 draws first \n",
    "# 1 draws second\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
    "# intersection = cv2.bitwise_and(image, eroded)\n",
    "cv2.imshow(\"Contours\", image)\n",
    "cv2.waitKey(0)\n",
    "# extract bouding coords\n",
    "c = max(contours, key=cv2.contourArea)\n",
    "extreme_pnts_left = tuple(c[c[:, :, 0].argmin()][0])\n",
    "extreme_pnts_right = tuple(c[c[:, :, 0].argmax()][0])\n",
    "extreme_pnts_top = tuple(c[c[:, :, 1].argmin()][0])\n",
    "extreme_pnts_bot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "new_image = test[extreme_pnts_top[1]:extreme_pnts_bot[1], extreme_pnts_left[0]:extreme_pnts_right[0]]\n",
    "new_image = cv2.resize(new_image, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "cv2.imshow(\"Cropped\", new_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "attached-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"../../assets/images/3190_left.jpg\")\n",
    "image = cv2.resize(image, None, fx=0.3, fy=0.3, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# red color boundaries [B, G, R]\n",
    "lower = [1, 0, 20]\n",
    "upper = [60, 40, 220]\n",
    "\n",
    "# create numpy arrays from the boundaries\n",
    "lower = np.array(lower, dtype='uint8')\n",
    "upper = np.array(upper, dtype='uint8')\n",
    "\n",
    "# find the colors within the specified boundaries and apply the mask\n",
    "mask = cv2.inRange(image, lower, upper)\n",
    "output = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "ret, thresh = cv2.threshold(mask, 40, 255, cv2.THRESH_BINARY)\n",
    "# if (cv2.__version__[0] > 3.0):\n",
    "#     contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# else:\n",
    "#     im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)    \n",
    "\n",
    "if len(contours) != 0:\n",
    "    # draw in blue the contours that were found\n",
    "    cv2.drawContours(output, contours, -1, (255, 0, 0), 3)\n",
    "    \n",
    "    # find the largest contour (c) by the area\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(c)\n",
    "    \n",
    "    # draw the largest contour (c) in green\n",
    "    cv2.rectangle(output, (x,y), (x+w, x+h), (0, 255, 0), 2)\n",
    "    \n",
    "# show the images\n",
    "cv2.imshow(\"Result\", np.hstack([image, output]))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "new_image = image[y:y+h, x:x+w]\n",
    "new_image = cv2.resize(new_image, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "cv2.imshow(\"Cropped\", new_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-prison",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOFT_ENG2",
   "language": "python",
   "name": "soft_eng2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
